### ğŸ“— çŸ­è¯­è¡¨ï¼ˆPhrasesï¼‰

| è‹±æ–‡çŸ­è¯­ | ä¸­æ–‡å«ä¹‰ | ç¤ºä¾‹ |
|----------|--------|------|
| data management | æ•°æ®ç®¡ç† | Efficient data management boosts performance. |
| training dataset | è®­ç»ƒæ•°æ®é›† | Construct a well-suited training dataset. |
| domain composition | é¢†åŸŸæ„æˆ | Domain composition affects reasoning ability. |
| task composition | ä»»åŠ¡æ„æˆ | Task composition influences multitask learning. |
| data quality control | æ•°æ®è´¨é‡æ§åˆ¶ | Quality control includes filtering and deduplication. |
| instruction dataset | æŒ‡ä»¤æ•°æ®é›† | SFT relies on carefully constructed instruction datasets. |
| model performance | æ¨¡å‹æ€§èƒ½ | Performance degrades with repeated data. |
| scaling laws | ç¼©æ”¾å®šå¾‹ | Scaling laws guide resource allocation. |
| compute budget | è®¡ç®—é¢„ç®— | Optimize under a fixed compute budget. |
| data exhaustion | æ•°æ®è€—å°½ | Data exhaustion drives synthesis research. |
| quality filtering | è´¨é‡è¿‡æ»¤ | Quality filtering removes noisy samples. |
| toxicity filtering | æœ‰å®³å†…å®¹è¿‡æ»¤ | Toxicity filtering reduces offensive outputs. |
| deduplication technique | å»é‡æŠ€æœ¯ | N-gram hashing is a common deduplication technique. |
| multitask fine-tuning | å¤šä»»åŠ¡å¾®è°ƒ | Multitask fine-tuning improves generalization. |
| instruction complexity | æŒ‡ä»¤å¤æ‚åº¦ | Higher complexity challenges reasoning. |
| instruction diversity | æŒ‡ä»¤å¤šæ ·æ€§ | Diversity prevents overfitting to formats. |
| data-efficient learning | æ•°æ®é«˜æ•ˆå­¦ä¹  | Dynamic selection enables data-efficient learning. |
| negative interference | è´Ÿé¢å¹²æ‰° | Mixing tasks causes negative interference. |
| gradient similarity | æ¢¯åº¦ç›¸ä¼¼æ€§ | Used to select relevant tasks. |
| representation similarity | è¡¨å¾ç›¸ä¼¼æ€§ | Measures task compatibility. |
| power-law relationship | å¹‚å¾‹å…³ç³» | Loss follows a power-law with data size. |
| optimal allocation | æœ€ä¼˜åˆ†é… | D_opt ~ C^0.54 is the optimal allocation. |
| temporal shift | æ—¶é—´åç§» | Temporal shift leads to inaccurate evaluation. |
| temporal misalignment | æ—¶é—´é”™ä½ | Temporal misalignment skews evaluation. |
| human-aligned | ä¸äººç±»å¯¹é½çš„ | Human-aligned models follow instructions better. |
| bias amplification | åè§æ”¾å¤§ | Synthetic data may cause bias amplification if unchecked. |
| emergent abilities | æ¶Œç°èƒ½åŠ› | Emergent abilities appear at scale. |
| downstream tasks | ä¸‹æ¸¸ä»»åŠ¡ | SFT improves performance on downstream tasks. |
| self-supervised pretraining | è‡ªç›‘ç£é¢„è®­ç»ƒ | LLMs rely on self-supervised pretraining. |
| deliberately curated | ç²¾å¿ƒç­–åˆ’çš„ | Deliberately curated data evokes alignment. |
| under-explored | å°šæœªå……åˆ†æ¢ç´¢çš„ | This area remains under-explored. |
| future directions | æœªæ¥æ–¹å‘ | We outline promising future directions. |
| synthetic data generation | åˆæˆæ•°æ®ç”Ÿæˆ | LLMs enable scalable synthetic data generation. |
| large language models (LLMs) | å¤§å‹è¯­è¨€æ¨¡å‹ | LLMs serve as universal data generators. |
| prompt engineering | æç¤ºå·¥ç¨‹ | Effective prompt engineering guides relevant outputs. |
| few-shot prompting | å°‘æ ·æœ¬æç¤º | Few-shot prompting yields task-specific examples. |
| zero-shot generation | é›¶æ ·æœ¬ç”Ÿæˆ | Zero-shot generation relies on model priors. |
| retrieval-augmented generation (RAG) | æ£€ç´¢å¢å¼ºç”Ÿæˆ | RAG grounds outputs in external knowledge. |
| self-instruct method | è‡ªæŒ‡ä»¤æ–¹æ³• | Self-Instruct bootstraps instruction-following data. |
| data scarcity | æ•°æ®ç¨€ç¼º | Data scarcity motivates synthetic alternatives. |
| low-resource settings | ä½èµ„æºåœºæ™¯ | Synthetic data excels in low-resource settings. |
| task-relevant examples | ä»»åŠ¡ç›¸å…³æ ·æœ¬ | LLMs produce artificial but task-relevant examples. |
| functional correctness | åŠŸèƒ½æ­£ç¡®æ€§ | Code must pass tests for functional correctness. |
| execution feedback | æ‰§è¡Œåé¦ˆ | Execution feedback validates synthetic programs. |
| distributional realism | åˆ†å¸ƒçœŸå®æ€§ | Lack of distributional realism limits utility. |
| distribution shift | åˆ†å¸ƒåç§» | Distribution shift occurs when synthetic â‰  real data. |
| quality assurance | è´¨é‡ä¿è¯ | Quality assurance includes filtering and validation. |
| post-generation filtering | ç”Ÿæˆåè¿‡æ»¤ | Post-generation filtering removes low-quality samples. |
| downstream task performance | ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ | Measured by accuracy on real test sets. |
| closed-loop generation | é—­ç¯ç”Ÿæˆ | Closed-loop pipelines risk model collapse. |
| model collapse | æ¨¡å‹å´©æºƒ | Recursive self-training risks model collapse. |
| self-reinforcement | è‡ªæˆ‘å¼ºåŒ– | Self-reinforcement amplifies initial biases. |
| curriculum learning | è¯¾ç¨‹å­¦ä¹  | Curriculum learning starts with easy synthetic tasks. |
| human-in-the-loop | äººåœ¨å›è·¯ | Human-in-the-loop validation improves reliability. |
| automated verification | è‡ªåŠ¨éªŒè¯ | Automated verification via unit tests. |
| unit test | å•å…ƒæµ‹è¯• | Only code passing all unit tests is retained. |
| code translation | ä»£ç ç¿»è¯‘ | LLMs generate Python-to-Java translation pairs. |
| bug repair | æ¼æ´ä¿®å¤ | Synthetic buggy/fixed pairs train repair models. |
| instruction-following models | æŒ‡ä»¤éµå¾ªæ¨¡å‹ | Fine-tuned using synthetic instruction datasets. |
| instruction tuning | æŒ‡ä»¤å¾®è°ƒ | Instruction tuning adapts LLMs to user tasks. |
| privacy preservation | éšç§ä¿æŠ¤ | Synthetic data aids privacy preservation. |
| cost-effectiveness | æˆæœ¬æ•ˆç›Š | Synthetic labeling is highly cost-effective. |
| empirical validation | å®è¯éªŒè¯ | Requires ablation studies and benchmarks. |
| statistical rigor | ç»Ÿè®¡ä¸¥è°¨æ€§ | Future work demands statistical rigor. |
| domain-specific nuances | é¢†åŸŸç‰¹å®šç»†å¾®å·®åˆ« | Code vs. text have different validation needs. |
| open research directions | å¼€æ”¾ç ”ç©¶æ–¹å‘ | Includes cross-modal synthesis and evaluation. |
| ethical safeguards | ä¼¦ç†ä¿éšœæªæ–½ | Needed to prevent misuse of synthetic data. |